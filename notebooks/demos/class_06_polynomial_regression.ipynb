{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classwork 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../utils/imports.py\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import *\n",
    "from utils.styles import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../utils/plotting.py\n",
    "import seaborn as sns\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import cufflinks as cf\n",
    "\n",
    "init_notebook_mode()\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Work through the following datasets, determining best fits for each data set (predictor value/y value in parens). To better evaluate or improve this process, try including:\n",
    "\n",
    "* Increase the number of training points N. This might give us a training set with more coverage, and lead to greater accuracy.\n",
    "* Increase the degree d of the polynomial. This might allow us to more closely fit the training data, and lead to a better result\n",
    "* Add more features. If we were to, for example, perform a linear regression using x, x√, x−1, or other functions, we might hit on a functional form which can better be mapped to the value of y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_DIR = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting stopping distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(DATA_DIR + 'cars1920.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore the features\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore the types of relationship you should model for, linear?\n",
    "df.iplot(x=\"speed\", y='dist', mode='markers', bestfit=True, shape='spline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(\"speed\", \"dist\", df, order=1, aspect=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore the types of relationship you should model for, second order polynomial?\n",
    "sns.lmplot(\"speed\", \"dist\", df, order=2, aspect=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore the types of relationship you should model for, third order polynomial?\n",
    "sns.lmplot(\"speed\", \"dist\", df, order=3, aspect=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is not strictly a linear relationship, we are going to use polynomial features. \n",
    "But there are two problems with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = [[i] for i in df.speed.values]\n",
    "y = df.dist\n",
    "\n",
    "\n",
    "scores = []\n",
    "for model in [Ridge, Lasso]:\n",
    "    for j in [0.1,0.5,1,2,5,10,20,100]: # ALPHA\n",
    "        for i in range(1,6):            # POLY\n",
    "            est = model(alpha=j)\n",
    "            poly = PolynomialFeatures(i)\n",
    "            X_poly = poly.fit_transform(X)\n",
    "            est.fit(X_poly,y)\n",
    "            scores.append((model.__name__, j, i, est.score(X_poly,y)))\n",
    "            \n",
    "df_scores = pd.DataFrame(scores, columns=['model','alpha','poly','score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal Polynomials - Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can generate highly correlated regressors by taking powers of `x`, leading to noisy parameter estimates. The input `x` are evenly space numbers on the interval `[0, 1]`. So `x` and $x^2$ are going to have a correlation over `95%`. Similar with $x^2$ and $x^3$. The solution to this is to use orthogonalized polynomial functions: tranformations of `x` that, when summed, result in polynomial functions, but are orthogonal (therefore uncorrelated) with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, we can easily calculate these transformations using patsy. The `C(x, Poly)` transform computes orthonormal polynomial functions of `x`, then we’ll extract out various orders of the polynomial. So `Xpoly[:, :2]` selects out the `0th` and 1st order functions, then when summed will give us a first order polynomial (i.e. linear). Similarly `Xpoly[: :4]` gives us the 0th through 3rd order functions, which sum up to a cubic polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math review : [Orthogonal Polynomials](http://mathoverflow.net/questions/38864/visualizing-orthogonal-polynomials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speed = df.speed.values\n",
    "dist = df.dist.values\n",
    "y = dist\n",
    "\n",
    "speed_poly = dmatrix('C(speed, Poly)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speed_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only Speed\n",
    "speed_poly1 = dmatrix('speed', df)\n",
    "\n",
    "# Only Speed^2\n",
    "speed_sq = dmatrix('np.power(speed,2)', df)\n",
    "\n",
    "# Speed + Speed^2\n",
    "speed_poly2 = dmatrix('speed + np.power(speed,2)')\n",
    "\n",
    "# Orthoganal Polynnomials for ^3, ^5 and ^25\n",
    "speed_poly3 = speed_poly[:, :4]\n",
    "speed_poly5 = speed_poly[:, :6]\n",
    "speed_poly25 = speed_poly[:, :26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xs = [speed_poly1, speed_sq, speed_poly2, speed_poly3, speed_poly5, speed_poly25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speed_poly2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see how closely they are correlated.\n",
    "pd.DataFrame(np.corrcoef(speed_poly25)).iplot(kind='heatmap',colorscale='spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we encounter now is how to choose what order polynomial to fit to the data. Any data can be fit well (i.e. have a high $R^2$) if we use a high enough order polynomial. But we will start to over-fit our data; capturing noise specific to our sample, leading to poor predictions on new data. The graph below shows the fits to the data of a straight line, a 3rd-order polynomial, a 5th-order polynomial, and a 25th-order polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speed_poly1_pred = sm.OLS(y, speed_poly1).fit().predict()\n",
    "speed_sq_pred = sm.OLS(y, speed_sq).fit().predict()\n",
    "speed_poly2_pred = sm.OLS(y, speed_poly2).fit().predict()\n",
    "speed_poly3_pred = sm.OLS(y, speed_poly3).fit().predict()\n",
    "speed_poly5_pred = sm.OLS(y, speed_poly5).fit().predict()\n",
    "speed_poly25_pred = sm.OLS(y, speed_poly25).fit().predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, sharex = True, sharey = True, figsize=(16,9))\n",
    "fig.subplots_adjust(hspace = 0.0, wspace = 0.0)\n",
    "fig.suptitle('Polynomial fits to stopping distance', fontsize = 16.0)\n",
    "\n",
    "# Iterate through panels (a), model predictions (p), and the polynomial \n",
    "# degree of the model (d). Plot the data, the predictions, and label\n",
    "# the graph to indicate the degree used.\n",
    "preds = [speed_poly1_pred, speed_sq_pred, speed_poly2_pred,\n",
    "         speed_poly3_pred, speed_poly5_pred, speed_poly25_pred]\n",
    "orders = ['1', 'sq', '2', '3', '5', '25']\n",
    "\n",
    "for a, p, d in zip(ax.ravel(), preds, orders):\n",
    "    a.plot(df.speed.values, df.dist.values, '.', color = 'steelblue', alpha = 0.5)\n",
    "    a.plot(df.speed.values, p)\n",
    "    a.text(.5, .95, 'D = ' + d, fontsize = 12,\n",
    "           verticalalignment = 'top',\n",
    "           horizontalalignment = 'center',\n",
    "           transform = a.transAxes)\n",
    "    a.grid()\n",
    "    \n",
    "# Alternate axes that have tick labels to avoid overlap.\n",
    "plt.setp(fig.axes[2].get_yaxis().get_ticklabels(), visible = False)\n",
    "plt.setp(fig.axes[3].get_yaxis(), ticks_position = 'right')   \n",
    "plt.setp(fig.axes[1].get_xaxis(), ticks_position = 'top')\n",
    "plt.setp(fig.axes[3].get_xaxis().get_ticklabels(), visible = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notice how the last fit gives us all kinds of degrees of freedom to capture specific datapoints, and the excessive “wiggles” look like we’re fitting to noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for order, X in zip(orders, Xs):\n",
    "    print \"%s : %.03f\" % (order, sm.OLS(y, X).fit().rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without cross-validation - which we'll be covering soon, it would appear that the `sq` model is preferred as that provides us with a smooth line, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The same with our SciKit Learn Models\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = [[i] for i in df.speed.values]\n",
    "y = df.dist\n",
    "\n",
    "scores = []\n",
    "for model in [Ridge, Lasso]:\n",
    "    for j in [0.1,0.5,1,2,5,10,20,100]: # ALPHA\n",
    "        for i in range(1,14):           # POLY\n",
    "            est = model(alpha=j)\n",
    "            poly = PolynomialFeatures(i)\n",
    "            X_poly = poly.fit_transform(X)\n",
    "            est.fit(X_poly,y)\n",
    "            scores.append((model.__name__, j, i, est.score(X_poly,y)))\n",
    "            \n",
    "df_scores = pd.DataFrame(scores, columns=['model','alpha','poly','score'])\n",
    "\n",
    "title = '<b>R-Squared for Linear Regression</b><br>With L1 and L2 Regularisation'\n",
    "yTitle = 'R-Squared'\n",
    "xTitle = 'Number of Polynominals'\n",
    "\n",
    "df_scores.groupby(['model','poly']).max().score.unstack(0).iplot(title=title,yTitle=yTitle,xTitle=xTitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try this with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores = []\n",
    "for model in [Ridge, Lasso]:\n",
    "    for j in [0.1,0.5,1,2,5,10,20,100]: # ALPHA\n",
    "        for i in range(1,10):           # POLY\n",
    "            est = model(alpha=j)\n",
    "            poly = PolynomialFeatures(i)\n",
    "            X_poly = poly.fit_transform(X)\n",
    "            \n",
    "            scores.append((model.__name__, j, i, cross_val_score(est, X_poly, y).mean()))\n",
    "            \n",
    "df_scores = pd.DataFrame(scores, columns=['model','alpha','poly','score'])\n",
    "\n",
    "title = '<b>R-Squared for Linear Regression</b><br>With L1 and L2 Regularisation'\n",
    "yTitle = 'R-Squared'\n",
    "xTitle = 'Number of Polynominals'\n",
    "\n",
    "df_scores.groupby(['model','poly']).max().score.unstack(0).iplot(title=title,yTitle=yTitle,xTitle=xTitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting City and Highway MPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "df = pd.read_csv(DATA_DIR + 'cars93.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Cylinders.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Make.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(df.Manufacturer).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if any of the columns marked as 'object' shouldn't be int or float instead\n",
    "objs = df.columns[df.dtypes == object]\n",
    "df[objs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[objs[:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yuhk - full stops in the column names, let's get rid of those first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = [col.replace('.','') for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hmmm why is the cylinder not an integer dtype?\n",
    "df.Cylinders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try coercing it into an int datatype\n",
    "try:\n",
    "    [int(v) for v in df.Cylinders if type(v)]\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ah, there's a row with a peculiar value\n",
    "df[df.Cylinders == 'rotary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's set the number of cylinders to 0\n",
    "df.loc[df.Cylinders == 'rotary', 'Cylinders'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change the Cylinder datatype\n",
    "df.Cylinders = df.Cylinders.astype(int)\n",
    "df.Cylinders.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "\n",
    "# Create a helper function to catch columns with any missing values\n",
    "# Logic : Check if there are any missing values, and sum them. True\n",
    "# counts as '1' so that's why this elegant solution works! \n",
    "is_null = lambda col: sum(pd.isnull(df[col]))\n",
    "\n",
    "# Logic : return (column name, missing value count) for each of the\n",
    "# columns in the dataframe, _if_ there are any missing values\n",
    "missing_values = [(col,is_null(col)) for col in df if is_null(col)]\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two columns have missing values, let's take a closer look at the data and figure out why that's the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rear.seat.room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "select = missing_values[0][0]\n",
    "df[df[select].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - so they are both sports cars. But let's sort by `Rear.seat.room` and see whether it's just that there _is_ no rear seat space, or whether the data is truly missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.sort_values('Rearseatroom')[['Manufacturer','Model','Type','Rearseatroom']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no cars where the `Rear.seat.room` is set to zero, so after a bit of Googling, you'd find that indeed these two cars don't allow for passengers in the back, so we can set these values to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crit = df['Rearseatroom'].isnull()\n",
    "df.loc[crit, 'Rearseatroom'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Luggage.room"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect the missing values for `Luggage.room`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "select = missing_values[1][0]\n",
    "df[df[select].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lot's of vans, and the two sports cars we just dealt with for missing Rear.seat.room values! Let's see how Vans \n",
    "are dealt with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df.Type == 'Van']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - turns out none of the vans are considered to have 'luggage room' , so let's also set ll these values to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crit = df['Luggageroom'].isnull()\n",
    "df.loc[crit, 'Luggageroom'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check to make sure we can move on now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_values = [(col,is_null(col)) for col in df if is_null(col)]\n",
    "len(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split out design matrix and response vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's build a model for MPG.city\n",
    "y = df['MPGcity']\n",
    "# y = df['MPG.highway']\n",
    "\n",
    "# Lazy selection of all numeric columns without MPG\n",
    "selection = df.select_dtypes(['int','float']).columns.difference(['MPGcity','MPGhighway'])\n",
    "X = df[selection]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a potential relationship between the predictors and the response variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_selection as fs\n",
    "\n",
    "def f_regression_feature_selection(input, response):    \n",
    "# use this against your feature matrix to determine p-values for\n",
    "# each feature (we care about the second array it returns).\n",
    "    return fs.univariate_selection.f_regression(input, response)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ValueError?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your attempt of running `f_regression_feature_selection` with a current `X` and `y` failed with the error:\n",
    "\n",
    "```bash\n",
    "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
    "```\n",
    "this meant that you likely didn't properly clean out the missing values from your design matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Exploration of Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(X,y).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.PairGrid(data, diag_sharey=False)\n",
    "g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "g.map_upper(plt.scatter)\n",
    "g.map_diag(sns.kdeplot, lw=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us something to work with, but there's no clear slam dunk. also, various features appear to have a curvilinear relation with MPG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"F value'' statistics tests the overall significance of the regression model.  Specifically, it tests the null hypothesis that all of the regression coefficients are equal to zero.  This tests the full model against a model with no variables and with the estimate of the dependent variable being the mean of the values of the dependent variable.  The F value is the ratio of the mean regression sum of squares divided by the mean error sum of squares.  Its value will range from zero to an arbitrarily large number.\n",
    "\n",
    "The null hypothesis is rejected if the F ratio is large. Some analysts recommend ignoring the P values for the individual regression coefficients if the overall F ratio is not statistically significant, because of the problems caused by multiple testing. Here we'll reject the features if they are smaller than 10.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many features don't meet the F test threshold?\n",
    "sum(f_regression_feature_selection(X,y)[0] < 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Which column are we talking about?\n",
    "select = f_regression_feature_selection(X,y)[0] < 10\n",
    "X.columns[select]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two columns we replaced the missing values! At this point I'm more than happy to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# difference between the ones available and the ones we wish to drop\n",
    "post_select = X.columns.difference(X.columns[select])\n",
    "\n",
    "Xs = X[post_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many features don't meet the F test threshold?\n",
    "sum(f_regression_feature_selection(Xs,y)[1] > 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! All of our predictors are significant ( p < 0.05 ) in a univariate regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The homework asked to select featured based on their P value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort the features based on their statistical significance \n",
    "ps = f_regression_feature_selection(Xs,y)[1]\n",
    "\n",
    "p_score = zip(Xs.columns, ps)\n",
    "ranked_p = sorted(p_score, key=lambda x:x[1])\n",
    "ranked_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we don't have the full toolkit yet to guard against overfitting - we'll introduce this in another lesson - but this is how you would loop through the various features and see the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's first build univariate models to see how well each individual features performs\n",
    "scores = []\n",
    "for feat, score in ranked_p:\n",
    "    est = Ridge()\n",
    "    X = [[x] for x in Xs[feat]]\n",
    "    est.fit(X,y)\n",
    "    scores.append(est.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop of R^2 with strong to weakest features\n",
    "cols = [k[0] for k in ranked_p]\n",
    "pd.DataFrame(scores,index=cols).iplot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 3 features seem to be reasonably powerful, after which the next 6 features are mediocre, and the remaining features aren't useful by themselves and will likely be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's build models which cummatively look well combinations of features performs\n",
    "scores = []\n",
    "feats = []\n",
    "for feat, score in ranked_p:\n",
    "    est = Ridge()\n",
    "    feats.append(feat)\n",
    "    if len(feats) == 1:\n",
    "        X = [[x] for x in Xs[feat]]\n",
    "    else:\n",
    "        X = Xs[feats]\n",
    "    est.fit(X, y)\n",
    "    scores.append(est.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop of R^2 with strong to weakest features\n",
    "# Drop of R^2 with strong to weakest features\n",
    "# cols = [k[0] for k in ranked_p]\n",
    "pd.DataFrame(scores,index=feats).iplot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first feature `Weight` is of course the most important, but the second and third features don't give us as much lift as we might have hoped for. This might have been due to the first three values being highly multicollinear. Let's inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlation = Xs[[x[0] for x in ranked_p][:3]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlation.iplot(kind='heatmap', colorscale='spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this with the correlation plot of the whole dataset, and indeed! Apart from price correlating with min- and max-price, these are some of the most highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.corrplot(Xs, annot=False, sig_stars=False,\n",
    "             diag_names=False, cmap=cmap, ax=ax)\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, based on the P values, the most important features are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_features = [x[0] for x in ranked_p][:5]\n",
    "p_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the the following plot, the feature which provide the largest improvement in R^2 are features [0,3,5], just out of interest, let's also build and score a model with only those features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop of R^2 with strong to weakest features\n",
    "plt.plot(range(len(scores)), scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "handpicked_features = [ranked_p[x][0] for x in [0,3,5]]\n",
    "print handpicked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est = Ridge()\n",
    "X = Xs[handpicked_features]\n",
    "est.fit(X, y).score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the addition of these features was pivotal for the previous cummalative model, when just taken alone "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have also decided to use [Recursive Feature Elimination](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)\n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and weights are assigned to each one of them. Then, features whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the RFE used our _coefficient size_ as a determinant of importance, we need to make sure we **standardise the data first**! This way the mean is always close to zero, and the standard deviation is one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stand_Xs = (Xs - Xs.mean()) / Xs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Create the RFE object and rank each features\n",
    "est = Ridge()\n",
    "rfe = RFE(estimator=est, n_features_to_select=1, step=1)\n",
    "\n",
    "rfe.fit(stand_Xs, y)\n",
    "ranking = rfe.ranking_\n",
    "\n",
    "scores = zip(Xs.columns,ranking)\n",
    "scores = sorted(scores, key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Feature Importance \n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.barplot(Xs.columns, 16 - ranking);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how did the automatic RFE do compared to our own more manual feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfe_features = [x[0] for x in scores][:5]\n",
    "print 'P Value:', p_features\n",
    "print 'RFE Value:', rfe_features\n",
    "print 'Handpicked Values:', handpicked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for feats in [handpicked_features, p_features, rfe_features]:\n",
    "    est = Ridge()\n",
    "    X = Xs[feats]\n",
    "    print est.fit(X, y).score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RFE feature selection delivered the strongest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynominal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will expand on polynominal feature selection once we introduce cross-validation, but for now let's use Ridge Regression to prevent against overfitting. Let's continue with the features found by the RFE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfe_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[poly_features.append(dmatrix('C('+ str(feat) +', Poly)', Xs)) for feat in rfe_features];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a new dataframe which has all the polynomial columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly_featurs_123 = [poly[:, 1:4] for poly in poly_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = poly_featurs_123\n",
    "\n",
    "dfs = [pd.DataFrame(p[x]) for x in range(5)]\n",
    "dfs = pd.concat(dfs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to rewrite the columns names since they got lost in the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_names = [name + '^' + str(e) for name in rfe_features for e in range(1,4)]\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, let's build a model with our 5 features all with 3rd order polynomimials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est = Ridge()\n",
    "print \"R^2 of %.02f based on %s features\" % (est.fit(dfs, y).score(dfs,y), len(dfs.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model didn't improve spectacularly, so until we are better equiped to evaluated our models, we ought to stick with the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use the `PolynomialFeatures` function from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(3)\n",
    "poly_Xs = poly.fit_transform(Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many features did we create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(poly_Xs) - len(Xs.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep - close to 100 features. Just for kicks, how close to 1 could we push our R^2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est = Ridge()\n",
    "print \"R^2 of %.02f based on %s features\" % (est.fit(poly_Xs, y).score(poly_Xs,y), len(poly_Xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep - perfect model - perfectly overfit that is! We'll see soon how to deal with this."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
