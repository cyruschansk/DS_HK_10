{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Random Forest Comic](https://s-media-cache-ak0.pinimg.com/736x/e9/c9/df/e9c9df5e60831e134d0dfa367bf286ce.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load utils/imports.py\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import *\n",
    "from utils.plotting import *\n",
    "\n",
    "from utils.demo import *\n",
    "from utils.styles import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests or random decision forests are an [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning) method for classification, regression, and other tasks. Ensemble methods are essentially machine learning methods that use multiple learning algorithms to obtain better predictive performance than could be obtained by using a single algorithm.\n",
    "\n",
    "They work by constructing many decision trees at training time and outputting the class that is the mode of the classes (for classification) or mean prediction (for regression) of the individual trees.\n",
    "\n",
    "Essentially, a random forest uses many small decision trees as weak learners in order to create a higher-level strong learner. The strong learner essentially polls its weak learners to 'vote' on which class (`y`) should be selected for a given input (`x`). The class with the most votes is selected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example Random Forest](http://image.slidesharecdn.com/mlsquare-140801092353-phpapp01/95/squares-machine-learning-infrastructure-and-applications-rong-yan-17-638.jpg?cb=1406885118)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strengths and Weaknesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strengths\n",
    "\n",
    "- Generally quite fast to build and train.\n",
    "- Does not expect linear features or even features that interact linearly.\n",
    "- Handle categorical features and continuous features very well.\n",
    "- Handle high dimensional spaces very well.\n",
    "- Scale well to a large number of instances since each individual tree is relatively small.\n",
    "- Often highly accurate in contrast to other models.\n",
    "\n",
    "### Weaknesses\n",
    "\n",
    "- Can be a bit slow to query, but it is generally fast enough in practice.\n",
    "- Can overfit on particularly noisy data.\n",
    "- Results of learning are hard to comprehend in contrast to decision trees.\n",
    "\n",
    "Scikit-learn provides the [`RandomForestClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and here is a quick example of how to use random forest on the motor trend dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn = 'mtcars.csv'\n",
    "download_data(fn)\n",
    "df = pd.read_csv('data/' + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "features = df.drop(['Car', 'gear'], axis=1)\n",
    "target = df.gear\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "scores = cross_val_score(rf, features, target)\n",
    "print(\"Accuracy: {:.2f} +/- {:.2f}\".format(scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "Random Forest is a fairly robust machine learning algorithm, but it is still prone to overfitting if given bad parameters. In particular, watch out when tweaking the following parameters to the `RandomForestClassifier`:\n",
    "- `n_estimators`: in general the more trees the less likely the algorithm is to overfit. The default is 10, but if you have a large amount of data you may want to try increasing this. The lower this number, the closer the model is to a decision tree, with a restricted feature set.\n",
    "- `max_features`: by default the algorithm uses the square root of the number of features. If you have a very large number of features, you may want to try adjusting this number (try 30-50% of the number of features). This determines how many features each tree is randomly assigned. The smaller, the less likely to overfit, but too small and it will start to introduce under fitting.\n",
    "- `max_depth`: Experiment with this. This will reduce the complexity of the learned models, lowering overfitting risk. Try starting small, say 5-10, and increasing you get the best result.\n",
    "- `min_samples_leaf`: Try setting this to values greater than one. This has a similar effect to the `max_depth parameter`. It means the branch will stop splitting once the leaves have that number of samples each."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
